{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3\n",
    "#### 学生信息：\n",
    "学号：24320172205050\n",
    "\n",
    "姓名：博雅力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1 (Infaraway)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Apriori1 import *\n",
    "\n",
    "def loaddat(file_name, flag = ' '):\n",
    "    dataSetDict = {}\n",
    "    dataSet = []\n",
    "    num_lines = sum(1 for line in open(file_name,'r'))\n",
    "    \n",
    "    print(\"reading data from\", file_name)\n",
    "    \n",
    "    with open(file_name,'r') as f:\n",
    "        for line in tqdm(f, total=num_lines):\n",
    "            line = line.strip().split(flag)\n",
    "            dataSet.append(line)\n",
    "            dataLine = [word for word in line]\n",
    "            dataSetDict[frozenset(dataLine)] = dataSetDict.get(frozenset(dataLine), 0) + 1\n",
    "    \n",
    "    print(\"done\")\n",
    "\n",
    "    return dataSetDict, dataSet, num_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_apriori(minSup, dataSetDict, dataSet):\n",
    "    freqItems = apriori_zc(dataSet, dataSetDict, minSup)\n",
    "    freqItems = sorted(freqItems.items(), key=lambda item: item[1])\n",
    "    return sorted(freqItems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kosarak_data_file = os.path.join('kosarak.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 27885/990002 [00:00<00:03, 278849.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data from kosarak.dat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990002/990002 [00:05<00:00, 165971.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataSetDict, dataSet, num_lines = loaddat(kosarak_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "minSup = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqItems_ap = test_apriori(minSup, dataSetDict, dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minSup : 5 data_num : 990002 freqItems_ap: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"minSup :\", minSup, \"data_num :\", num_lines ,\"freqItems_ap:\", len(freqItems_ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sup = 0.2 * num_lines\n",
    "freqItems = apriori_zc(dataSet, dataSetDict, 0.1)\n",
    "freqItems = sorted(freqItems.items(), key=lambda item: item[1], reverse=True)\n",
    "max = 0\n",
    "index = 0\n",
    "\n",
    "for i, item in enumerate(freqItems):\n",
    "    if len(item[0]) > max:\n",
    "        max = len(item[0])\n",
    "        index = i\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(frozenset({'1'}), 1), (frozenset({'2'}), 1), (frozenset({'3'}), 1)]\n"
     ]
    }
   ],
   "source": [
    "print(freqItems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kosarak_data_file = os.path.join('kosarak.dat')\n",
    "num_lines = sum(1 for line in open(kosarak_data_file,'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite hard to use implementation from Infaraway, let's try some other implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/990002 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from file: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990002/990002 [00:06<00:00, 148846.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('reading from file: ')\n",
    "loaded_list_list = []\n",
    "\n",
    "with open(kosarak_data_file,'r') as f:\n",
    "    for line in tqdm(f, total=num_lines):\n",
    "        temp_list = []\n",
    "#         temp_tup = (word for word in line)\n",
    "        for x in line:\n",
    "            if x != '\\n' and x != ' ':\n",
    "                temp_list.append(x)\n",
    "        loaded_list_list.append(temp_list)\n",
    "        \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  Apriori import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[frozenset({'0'}),\n",
       " frozenset({'1'}),\n",
       " frozenset({'2'}),\n",
       " frozenset({'3'}),\n",
       " frozenset({'4'}),\n",
       " frozenset({'5'}),\n",
       " frozenset({'6'}),\n",
       " frozenset({'7'}),\n",
       " frozenset({'8'}),\n",
       " frozenset({'9'})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C1 = createC1(loaded_list_list)\n",
    "C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = list(map(set,loaded_list_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[frozenset({'6'}),\n",
       " frozenset({'4'}),\n",
       " frozenset({'3'}),\n",
       " frozenset({'2'}),\n",
       " frozenset({'1'})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L1, suppDat0 = scanD(D,C1,0.5)\n",
    "L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'6'}) --> frozenset({'1'}) conf: 0.7868650084175777\n",
      "frozenset({'1'}) --> frozenset({'6'}) conf: 0.7957586387427283\n",
      "frozenset({'6'}) --> frozenset({'3'}) conf: 0.7490765381647209\n",
      "frozenset({'3'}) --> frozenset({'6'}) conf: 0.7599111241128083\n",
      "frozenset({'3'}) --> frozenset({'1'}) conf: 0.761323665642783\n",
      "frozenset({'1'}) --> frozenset({'3'}) conf: 0.7589511997655878\n",
      "CPU times: user 7.83 s, sys: 215 ms, total: 8.05 s\n",
      "Wall time: 8.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "L, suppData = apriori(loaded_list_list, minSupport=0.5)\n",
    "rules = generateRules(L, suppData, minConf=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Apriori\n",
    "**Source:** [tommyod](https://github.com/tommyod/Efficient-Apriori)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    The classic apriori algorithm as described in 1994 by Agrawal et al.\n",
    "    \n",
    "    The Apriori algorithm works in two phases. Phase 1 iterates over the \n",
    "    transactions several times to build up itemsets of the desired support\n",
    "    level. Phase 2 builds association rules of the desired confidence given the\n",
    "    itemsets found in Phase 1. Both of these phases may be correctly\n",
    "    implemented by exhausting the search space, i.e. generating every possible\n",
    "    itemset and checking it's support. The Apriori prunes the search space\n",
    "    efficiently by deciding apriori if an itemset possibly has the desired\n",
    "    support, before iterating over the entire dataset and checking.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    transactions : list of tuples, list of itemsets.TransactionWithId,\n",
    "        or a callable returning a generator. Use TransactionWithId's when\n",
    "        the transactions have ids which should appear in the outputs.\n",
    "        The transactions may be either a list of tuples, where the tuples must\n",
    "        contain hashable items. Alternatively, a callable returning a generator\n",
    "        may be passed. A generator is not sufficient, since the algorithm will\n",
    "        exhaust it, and it needs to iterate over it several times. Therefore,\n",
    "        a callable returning a generator must be passed.\n",
    "    min_support : float\n",
    "        The minimum support of the rules returned. The support is frequency of\n",
    "        which the items in the rule appear together in the data set.\n",
    "    min_confidence : float\n",
    "        The minimum confidence of the rules returned. Given a rule X -> Y, the\n",
    "        confidence is the probability of Y, given X, i.e. P(Y|X) = conf(X -> Y)\n",
    "    max_length : int\n",
    "        The maximum length of the itemsets and the rules.\n",
    "    verbosity : int\n",
    "        The level of detail printing when the algorithm runs. Either 0, 1 or 2.\n",
    "    output_transaction_ids : bool\n",
    "        If set to true, the output contains the ids of transactions that\n",
    "        contain a frequent itemset. The ids are the enumeration of the\n",
    "        transactions in the sequence they appear.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> transactions = [('a', 'b', 'c'), ('a', 'b', 'd'), ('f', 'b', 'g')]\n",
    "    >>> itemsets, rules = apriori(transactions, min_confidence=1)\n",
    "    >>> rules\n",
    "    [{a} -> {b}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "from efficient_apriori.itemsets import itemsets_from_transactions, ItemsetCount, generate_rules_apriori\n",
    "# from efficient_apriori.rules import generate_rules_apriori\n",
    "\n",
    "# >>> transactions = [('a', 'b', 'c'), ('a', 'b', 'd'), ('f', 'b', 'g')]\n",
    "# >>> itemsets, rules = apriori(transactions, min_confidence=1)\n",
    "\n",
    "def apriori(\n",
    "    transactions: typing.Union[typing.List[tuple], typing.Callable],\n",
    "    min_support: float = 0.5,\n",
    "    min_confidence: float = 0.5,\n",
    "    max_length: int = 8,\n",
    "    verbosity: int = 0,\n",
    "    output_transaction_ids: bool = False,\n",
    "):\n",
    "\n",
    "    itemsets, num_trans = itemsets_from_transactions(\n",
    "        transactions,\n",
    "        min_support,\n",
    "        max_length,\n",
    "        verbosity,\n",
    "        output_transaction_ids,\n",
    "    )\n",
    "\n",
    "    if itemsets and isinstance(next(iter(itemsets[1].values())), ItemsetCount):\n",
    "        itemsets_for_rules = _convert_to_counts(itemsets)\n",
    "    else:\n",
    "        itemsets_for_rules = itemsets\n",
    "\n",
    "    rules = generate_rules_apriori(\n",
    "        itemsets_for_rules, min_confidence, num_trans, verbosity\n",
    "    )\n",
    "    return itemsets, list(rules)\n",
    "\n",
    "\n",
    "def _convert_to_counts(itemsets):\n",
    "    itemsets_counts = {}\n",
    "    for size, sets in itemsets.items():\n",
    "        itemsets_counts[size] = {i: c.itemset_count for i, c in sets.items()}\n",
    "    return itemsets_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 17018/990002 [00:00<00:12, 80974.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from file: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990002/990002 [00:12<00:00, 77369.00it/s]\n"
     ]
    }
   ],
   "source": [
    "loaded_list_tuples = []\n",
    "\n",
    "print('reading from file: ')\n",
    "\n",
    "with open(kosarak_data_file,'r') as f:\n",
    "    for line in tqdm(f, total=num_lines):\n",
    "        temp_tup = ()\n",
    "#         temp_tup = (word for word in line)\n",
    "        for x in line:\n",
    "            if x != '\\n' and x != ' ':\n",
    "                temp_tup = temp_tup + (x, )\n",
    "        loaded_list_tuples.append(temp_tup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the same arguments, so we can compare time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.1 s, sys: 47 ms, total: 4.15 s\n",
      "Wall time: 4.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "itemsets, rules = apriori(loaded_list_tuples, min_confidence=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3} -> {1} (conf: 0.761, supp: 0.552, lift: 1.047, conv: 1.142)\n",
      "{1} -> {3} (conf: 0.759, supp: 0.552, lift: 1.047, conv: 1.140)\n",
      "{6} -> {1} (conf: 0.787, supp: 0.579, lift: 1.082, conv: 1.279)\n",
      "{1} -> {6} (conf: 0.796, supp: 0.579, lift: 1.082, conv: 1.295)\n",
      "{6} -> {3} (conf: 0.749, supp: 0.551, lift: 1.033, conv: 1.096)\n",
      "{3} -> {6} (conf: 0.760, supp: 0.551, lift: 1.033, conv: 1.101)\n"
     ]
    }
   ],
   "source": [
    "for x in  rules :\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efficient implementation is upto 100% faster, than a previous(simpler) implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FPtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % conda install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pasta</th>\n",
       "      <th>milk</th>\n",
       "      <th>water</th>\n",
       "      <th>biscuits</th>\n",
       "      <th>coffee</th>\n",
       "      <th>brioches</th>\n",
       "      <th>yoghurt</th>\n",
       "      <th>frozen vegetables</th>\n",
       "      <th>tunny</th>\n",
       "      <th>beer</th>\n",
       "      <th>tomato souce</th>\n",
       "      <th>coke</th>\n",
       "      <th>rice</th>\n",
       "      <th>juices</th>\n",
       "      <th>crackers</th>\n",
       "      <th>oil</th>\n",
       "      <th>frozen fish</th>\n",
       "      <th>ice cream</th>\n",
       "      <th>mozzarella</th>\n",
       "      <th>tinned meat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pasta  milk  water  biscuits  coffee  brioches  yoghurt  frozen vegetables  \\\n",
       "0      0     0      0         0       0         0        0                  0   \n",
       "1      0     1      0         0       0         0        0                  0   \n",
       "2      1     1      0         0       0         0        0                  0   \n",
       "3      0     0      1         0       0         0        0                  0   \n",
       "4      0     0      0         0       0         0        0                  0   \n",
       "\n",
       "   tunny  beer  tomato souce  coke  rice  juices  crackers  oil  frozen fish  \\\n",
       "0      1     0             0     0     0       0         0    0            0   \n",
       "1      0     0             0     0     0       0         0    0            0   \n",
       "2      0     0             0     1     0       0         1    0            0   \n",
       "3      0     0             0     0     0       0         0    0            0   \n",
       "4      1     0             0     0     0       0         0    0            0   \n",
       "\n",
       "   ice cream  mozzarella  tinned meat  \n",
       "0          0           1            1  \n",
       "1          0           0            0  \n",
       "2          0           0            0  \n",
       "3          0           0            0  \n",
       "4          0           0            0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('Transactions.xls', dtype='int8')  # can be bool, harder to read though\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 20 columns):\n",
      " #   Column             Non-Null Count  Dtype\n",
      "---  ------             --------------  -----\n",
      " 0   pasta              10000 non-null  int8 \n",
      " 1   milk               10000 non-null  int8 \n",
      " 2   water              10000 non-null  int8 \n",
      " 3   biscuits           10000 non-null  int8 \n",
      " 4   coffee             10000 non-null  int8 \n",
      " 5   brioches           10000 non-null  int8 \n",
      " 6   yoghurt            10000 non-null  int8 \n",
      " 7   frozen vegetables  10000 non-null  int8 \n",
      " 8   tunny              10000 non-null  int8 \n",
      " 9   beer               10000 non-null  int8 \n",
      " 10  tomato souce       10000 non-null  int8 \n",
      " 11  coke               10000 non-null  int8 \n",
      " 12  rice               10000 non-null  int8 \n",
      " 13  juices             10000 non-null  int8 \n",
      " 14  crackers           10000 non-null  int8 \n",
      " 15  oil                10000 non-null  int8 \n",
      " 16  frozen fish        10000 non-null  int8 \n",
      " 17  ice cream          10000 non-null  int8 \n",
      " 18  mozzarella         10000 non-null  int8 \n",
      " 19  tinned meat        10000 non-null  int8 \n",
      "dtypes: int8(20)\n",
      "memory usage: 195.4 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTransection = []\n",
    "datalen = len(df)\n",
    "totalCol = len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# totalCol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "making basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, datalen):\n",
    "    allTransection.append([str(df.values[i, j]) for j in range(0, totalCol)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allTransection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpgrowth import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "CPU times: user 14.2 s, sys: 45 ms, total: 14.3 s\n",
      "Wall time: 14.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pattern = find_frequent_patterns( allTransection, 500 )\n",
    "rules   = generate_association_rules( pattern, 0.8 ) \n",
    "\n",
    "print(len(pattern))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing most common pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('1', '1', '1', '1', '1', '1', '1', '1', '1'): 1476,\n",
       " ('0', '1', '1', '1', '1', '1', '1', '1', '1', '1'): 11806,\n",
       " ('0', '1', '1', '1', '1', '1', '1', '1', '1'): 31896,\n",
       " ('0', '0', '1', '1', '1', '1', '1', '1', '1', '1'): 122301,\n",
       " ('0', '1', '1', '1', '1', '1', '1', '1'): 69382,\n",
       " ('0', '0', '1', '1', '1', '1', '1', '1', '1'): 288708,\n",
       " ('0', '1', '1', '1', '1', '1', '1'): 128114,\n",
       " ('0', '0', '1', '1', '1', '1', '1', '1'): 589904,\n",
       " ('0', '1', '1', '1', '1', '1'): 209682,\n",
       " ('0', '0', '1', '1', '1', '1', '1'): 1083432,\n",
       " ('0', '1', '1', '1', '1'): 308934,\n",
       " ('0', '0', '1', '1', '1', '1'): 1792800,\n",
       " ('0', '1', '1', '1'): 403772,\n",
       " ('0', '0', '1', '1', '1'): 2612308,\n",
       " ('0', '1', '1'): 452454,\n",
       " ('0', '0', '1', '1'): 3240201,\n",
       " ('0', '1'): 405022,\n",
       " ('0', '0', '1'): 3192744,\n",
       " ('0',): 175516,\n",
       " ('0', '0'): 1464891}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', '1', '1', '1', '1', '1', '1', '1', '1')  =>  (('0',), 7.9986449864498645)\n",
      "('0', '1', '1', '1', '1', '1', '1')  =>  ((), 4.604524095727243)\n",
      "('0', '1', '1', '1', '1', '1', '1', '1')  =>  ((), 4.161136894295351)\n",
      "('0', '1', '1', '1', '1', '1', '1', '1', '1')  =>  ((), 3.834367945823928)\n",
      "('0',)  =>  ((), 8.346196358166777)\n",
      "('0', '1', '1', '1', '1')  =>  ((), 5.80318126201713)\n",
      "('0', '1', '1', '1', '1', '1')  =>  ((), 5.167024351160329)\n",
      "('0', '1')  =>  ((), 7.882890312131193)\n",
      "('0', '1', '1')  =>  ((), 7.161393202402897)\n",
      "('0', '1', '1', '1')  =>  ((), 6.469760161675401)\n",
      "('0', '0')  =>  (('1',), 2.1795096017382862)\n",
      "('0', '0', '1')  =>  ((), 1.0148640166577716)\n",
      "('0', '0', '1', '1')  =>  ((), 0.8062178858657225)\n"
     ]
    }
   ],
   "source": [
    "for i in rules:\n",
    "    print(i, ' => ', rules[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| FP Growth | Apriori|\n",
    "| ------------- |: ------------- :|\n",
    "**Pattern Generation**\n",
    "| FP growth generates pattern by constructing a FP tree | Apriori generates pattern by pairing the items into singletons, pairs and triplets.|\n",
    "**Candidate Generation**\n",
    "| There is no candidate generation | Apriori uses candidate generation |\n",
    "**Process**\n",
    "The process is faster as compared to Apriori. The runtime of process increases linearly with increase in number of itemsets. | The process is comparatively slower than FP Growth, the runtime increases exponentially with increase in number of itemsets\n",
    "**Memory Usage**\n",
    "A compact version of database is saved | The candidates combinations are saved in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-hwcg",
   "language": "python",
   "name": "py3-hwcg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
